#!/usr/bin/env python3
"""
CNNäººè„¸è¯†åˆ«æ¨¡å‹è®­ç»ƒè„šæœ¬
ç”¨äºPCç«¯é¢„è®­ç»ƒï¼Œç„¶åéƒ¨ç½²åˆ°MaixCAM
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
import torchvision.models as models
from torch.utils.data import Dataset, DataLoader
import os
import json
import numpy as np
from PIL import Image
import argparse

class FaceDataset(Dataset):
    """äººè„¸æ•°æ®é›†ç±»"""
    
    def __init__(self, data_dir, transform=None):
        self.data_dir = data_dir
        self.transform = transform
        self.images = []
        self.labels = []
        self.label_to_name = {}
        
        # æ‰«ææ•°æ®ç›®å½•
        label_id = 0
        for person_name in os.listdir(data_dir):
            person_dir = os.path.join(data_dir, person_name)
            if os.path.isdir(person_dir):
                self.label_to_name[label_id] = person_name
                
                for img_file in os.listdir(person_dir):
                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):
                        img_path = os.path.join(person_dir, img_file)
                        self.images.append(img_path)
                        self.labels.append(label_id)
                
                label_id += 1
        
        print(f"âœ“ åŠ è½½æ•°æ®é›†: {len(self.images)} å¼ å›¾ç‰‡, {len(self.label_to_name)} ä¸ªäººç‰©")
        for label_id, name in self.label_to_name.items():
            count = self.labels.count(label_id)
            print(f"  - {name}: {count} å¼ å›¾ç‰‡")
    
    def __len__(self):
        return len(self.images)
    
    def __getitem__(self, idx):
        img_path = self.images[idx]
        label = self.labels[idx]
        
        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        
        if self.transform:
            image = self.transform(image)
        
        return image, label

class FaceEncoder(nn.Module):
    """è½»é‡åŒ–äººè„¸ç‰¹å¾ç¼–ç å™¨"""
    
    def __init__(self, num_classes, feature_dim=128):
        super(FaceEncoder, self).__init__()
        
        # ä½¿ç”¨é¢„è®­ç»ƒçš„MobileNetV2ä½œä¸ºéª¨å¹²ç½‘ç»œ
        self.backbone = models.mobilenet_v2(pretrained=True)
        
        # æ›¿æ¢åˆ†ç±»å¤´
        self.backbone.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(1280, feature_dim),
            nn.ReLU(inplace=True),
            nn.Linear(feature_dim, num_classes)
        )
        
        # ç‰¹å¾æå–å™¨ï¼ˆç”¨äºéƒ¨ç½²ï¼‰
        self.feature_extractor = nn.Sequential(
            self.backbone.features,
            nn.AdaptiveAvgPool2d((1, 1)),
            nn.Flatten(),
            nn.Dropout(0.2),
            nn.Linear(1280, feature_dim)
        )
    
    def forward(self, x):
        return self.backbone(x)
    
    def extract_features(self, x):
        """æå–ç‰¹å¾å‘é‡ï¼ˆç”¨äºç›¸ä¼¼åº¦è®¡ç®—ï¼‰"""
        features = self.feature_extractor(x)
        return F.normalize(features, p=2, dim=1)

def create_data_transforms():
    """åˆ›å»ºæ•°æ®å˜æ¢"""
    train_transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.RandomHorizontalFlip(p=0.3),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(brightness=0.2, contrast=0.2),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    val_transform = transforms.Compose([
        transforms.Resize((64, 64)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                           std=[0.229, 0.224, 0.225])
    ])
    
    return train_transform, val_transform

def train_model(data_dir, output_dir, epochs=50, batch_size=16, lr=0.001):
    """è®­ç»ƒæ¨¡å‹"""
    
    print("ğŸš€ å¼€å§‹è®­ç»ƒCNNäººè„¸è¯†åˆ«æ¨¡å‹...")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # æ•°æ®å˜æ¢
    train_transform, val_transform = create_data_transforms()
    
    # åŠ è½½æ•°æ®é›†
    dataset = FaceDataset(data_dir, train_transform)
    
    # æ•°æ®é›†åˆ’åˆ†
    train_size = int(0.8 * len(dataset))
    val_size = len(dataset) - train_size
    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])
    
    # ä¸ºéªŒè¯é›†åº”ç”¨ä¸åŒçš„å˜æ¢
    val_dataset.dataset.transform = val_transform
    
    # æ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
    
    # æ¨¡å‹
    num_classes = len(dataset.label_to_name)
    model = FaceEncoder(num_classes)
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {device}")
    
    # æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)
    
    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0
    
    for epoch in range(epochs):
        # è®­ç»ƒé˜¶æ®µ
        model.train()
        train_loss = 0.0
        train_correct = 0
        
        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)
            
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            pred = output.argmax(dim=1, keepdim=True)
            train_correct += pred.eq(target.view_as(pred)).sum().item()
        
        # éªŒè¯é˜¶æ®µ
        model.eval()
        val_loss = 0.0
        val_correct = 0
        
        with torch.no_grad():
            for data, target in val_loader:
                data, target = data.to(device), target.to(device)
                output = model(data)
                val_loss += criterion(output, target).item()
                pred = output.argmax(dim=1, keepdim=True)
                val_correct += pred.eq(target.view_as(pred)).sum().item()
        
        # è®¡ç®—å‡†ç¡®ç‡
        train_acc = 100. * train_correct / len(train_dataset)
        val_acc = 100. * val_correct / len(val_dataset)
        
        print(f'Epoch {epoch+1}/{epochs}:')
        print(f'  è®­ç»ƒ - Loss: {train_loss/len(train_loader):.4f}, Acc: {train_acc:.2f}%')
        print(f'  éªŒè¯ - Loss: {val_loss/len(val_loader):.4f}, Acc: {val_acc:.2f}%')
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save({
                'model_state_dict': model.state_dict(),
                'label_to_name': dataset.label_to_name,
                'feature_dim': 128,
                'num_classes': num_classes
            }, os.path.join(output_dir, 'best_face_model.pth'))
            print(f'  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%)')
        
        scheduler.step()
        print()
    
    print(f"ğŸ¯ è®­ç»ƒå®Œæˆ! æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.2f}%")
    
    # ä¿å­˜æ ‡ç­¾æ˜ å°„
    with open(os.path.join(output_dir, 'label_mapping.json'), 'w') as f:
        json.dump(dataset.label_to_name, f, indent=2, ensure_ascii=False)
    
    return model, dataset.label_to_name

def export_for_maixpy(model_path, output_dir):
    """å¯¼å‡ºæ¨¡å‹ç”¨äºMaixPyéƒ¨ç½²"""
    
    print("ğŸ“¦ å¯¼å‡ºæ¨¡å‹ç”¨äºMaixPyéƒ¨ç½²...")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹
    checkpoint = torch.load(model_path, map_location='cpu')
    
    model = FaceEncoder(checkpoint['num_classes'], checkpoint['feature_dim'])
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # åˆ›å»ºç¤ºä¾‹è¾“å…¥
    dummy_input = torch.randn(1, 3, 64, 64)
    
    # å¯¼å‡ºONNXæ¨¡å‹
    onnx_path = os.path.join(output_dir, 'face_encoder.onnx')
    torch.onnx.export(
        model.feature_extractor,  # åªå¯¼å‡ºç‰¹å¾æå–éƒ¨åˆ†
        dummy_input,
        onnx_path,
        export_params=True,
        opset_version=11,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['features'],
        dynamic_axes={'input': {0: 'batch_size'}, 'features': {0: 'batch_size'}}
    )
    
    print(f"âœ“ ONNXæ¨¡å‹å·²ä¿å­˜: {onnx_path}")
    print("ğŸ“ ä¸‹ä¸€æ­¥:")
    print("  1. å°†ONNXæ¨¡å‹è½¬æ¢ä¸ºMaixPyæ”¯æŒçš„.mudæ ¼å¼")
    print("  2. å°†æ¨¡å‹æ–‡ä»¶å¤åˆ¶åˆ°MaixCAMçš„/root/models/ç›®å½•")
    print("  3. æ›´æ–°MaixPyä»£ç ä»¥ä½¿ç”¨CNNç‰¹å¾æå–")

def main():
    parser = argparse.ArgumentParser(description='è®­ç»ƒCNNäººè„¸è¯†åˆ«æ¨¡å‹')
    parser.add_argument('--data_dir', type=str, required=True,
                      help='è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./trained_models',
                      help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--epochs', type=int, default=50,
                      help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=16,
                      help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--lr', type=float, default=0.001,
                      help='å­¦ä¹ ç‡')
    
    args = parser.parse_args()
    
    # è®­ç»ƒæ¨¡å‹
    model, label_mapping = train_model(
        args.data_dir, 
        args.output_dir, 
        args.epochs, 
        args.batch_size, 
        args.lr
    )
    
    # å¯¼å‡ºç”¨äºéƒ¨ç½²
    model_path = os.path.join(args.output_dir, 'best_face_model.pth')
    export_for_maixpy(model_path, args.output_dir)

if __name__ == '__main__':
    main()
